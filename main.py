from train import run_train
from validate import run_evaluate
import run_split
from deepforest import main
from predict import process_all_tif_files_in_folder  # Import the function from predict.py
import warnings
warnings.filterwarnings("ignore")

# Define run_split arguments
args_split = {
    "annotations": [  # List of paths to shapefile annotations
        r"Path\to\shapefile.shp",
        r"Path\to\shapefile.shp"
    ],
    "image_path": [  # List of paths to corresponding TIFF images
         r"Path\to\raster.tif",
         r"Path\to\raster.tif"
    ],
    "directory_to_save_crops": r"Path\to\output_folder",  # Directory to save cropped image tiles
    "patch_size": 400,  # Size of each crop (e.g., `400x400` pixels)
    "patch_overlap": 0.0,  # Overlap percentage between cropped tiles
    "merge_name": "csv_ref_merged.csv",  # Name of the merged output CSV file
    "split": 0.3,  # Percentage of data to allocate to the test set between [0.0, 1.0]
    "seed": 42,  # Random seed for reproducibility of the split
    "label": {'Tree': 0},  # Mapping of label names to numerical IDs
    "max_empty": 0.2  # Maximum proportion of empty tiles allowed
}

# Define train arguments
args_train = {
    "epochs": 50,  # Total number of epochs to train the model
    "valid_every_n_epochs": 1,  # Perform validation after every n epochs
    "save_top_k": -1, # Save model checkpoints every n epochs ( -1 saved the last and best accuracy model)
    "augmentation_ratio": 1,  # Probability of applying data augmentation
    "batch_size": 4,  # Number of samples per batch
    "monitor": "iou",  # Metric to monitor during training: 'val_classification' or 'iou'
    "score_thresh": 0.4,  # Score threshold for filtering predictions
    "nms_thresh": 0.05,  # Non-Max Suppression (NMS) threshold for overlapping predictions
    "optimizer_type": "SGD",  # Optimizer type (e.g., 'SGD', 'Adam', 'AdamW')
    "learning_rate": 0.001,  # Initial learning rate for the optimizer
    "optimizer_patience": 20,  # Number of epochs with no improvement to wait before reducing LR
    "model_save_dir": r"Q:\MnD\projects\2024_11_01_object_detection\Deepforest\github_script\checkpoints",  # Directory to save model checkpoints
    "tb_log_dir": "tb_logs/deepforest",  # Directory to save TensorBoard logs
    "train_csv": r"Q:\MnD\projects\2024_11_01_object_detection\Deepforest\github_script\test\train_csv_ref_merged.csv",  # Path to the training dataset CSV file
    "val_csv": r"Q:\MnD\projects\2024_11_01_object_detection\Deepforest\github_script\test\test_csv_ref_merged.csv",  # Path to the validation dataset CSV file
    "tensorboard_port": 6006,  # Port for TensorBoard logging
    "default_label": "Tree"  # Default label for missing values in the dataset
}


# Define evaluation arguments
args_eval = {
    "model_checkpoints": [  # List of model checkpoint paths to evaluate or just the best one
       r"Path\to\best_model-epoch01-val_classification0.2652.ckpt"
    ],
    "evaluation_csv": r"Path\to\val_csv_ref_merged.csv",  # CSV file containing test dataset information
    "evaluation_root_dir": r"PAth\to\tile_images\folder",  # Root directory for evaluation data
    "predictions_save_dir": r"Path\to\evaluation\predictions\folder",  # Directory to save evaluation predictions
}


# Define prediction arguments
args_predict = {
    "model_path": r"Path\to\model_saved\best_model-epoch01-val_classification0.2652.ckpt",  # Model path for prediction
    "folder_path": r"Path\to\target\folder",  # Folder containing TIFF files for prediction
    "savedir": r"Path\to\save\Shapefiles\predictions",  # Directory to save shapefiles generated by predictions
    "small_tiles": True,  # Whether to use tile-based prediction for large images, False if the image is small 
    "patch_size": 400,    # Size of each tile used during prediction
    "patch_overlap": 0.15,  # Overlap percentage between adjacent tiles
    "iou_threshold": 0.4,   # Intersection over Union (IoU) threshold for filtering predictions
    "thresh": 0.4  # Confidence score threshold for filtering predictions
}


# load the predict model
def run_predict(args_predict):
    """
    Main function to execute the prediction pipeline for all TIFF files in a folder.
    """
    model_path = args_predict["model_path"]
    model = main.deepforest.load_from_checkpoint(model_path)  # Load the model once

    process_all_tif_files_in_folder(
        model=model,
        folder_path=args_predict["folder_path"],
        savedir=args_predict["savedir"],
        small_tiles=args_predict["small_tiles"],
        patch_size=args_predict["patch_size"],
        patch_overlap=args_predict["patch_overlap"],
        iou_threshold=args_predict["iou_threshold"],
        thresh=args_predict["thresh"]
    )

# define the task
split_raster = False
train = True
vlid = False
predict = False


if __name__ == "__main__":
        if split_raster:
            print(r"\nStarting data preprocessing and split pipeline...")
            # Call the function in run_split.py using args_split
            run_split.preprocess(**args_split)
            print("Data preprocessing and split completed.")
    
        # Handle other processes like training, validation, and prediction here (if needed)
        if train:
            print("Starting training pipeline...")
            run_train(args_train)
            print("Training completed.")
        
        if vlid:
            print("Starting evaluation pipeline...")
            run_evaluate(args_eval)
            print("Evaluation completed.")
        
        if predict:
            print("Starting prediction pipeline...")
            run_predict(args_predict)
            print("Prediction completed.")
    
        print("Script finished.")
